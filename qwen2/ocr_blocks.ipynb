{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForImageTextToText, TrainingArguments, Trainer, AutoProcessor,  BitsAndBytesConfig\n",
    "from datasets import Dataset, load_dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "import logging\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8255f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def create_amazigh_alphabet_prompt():\n",
    "    \"\"\"Create a system prompt that includes Amazigh alphabet information\"\"\"\n",
    "    return \"\"\"You are an expert OCR system specialized in recognizing Amazigh (Berber) text. \n",
    "    You can read text in multiple scripts used for Amazigh languages:\n",
    "    \n",
    "    1. Tifinagh script: ⴰⴱⴳⴷⴹⴻⴼⴳⵀⵃⵄⵅⵇⵈⵉⵊⵋⵍⵎⵏⵓⵔⵕⵖⵗⵘⵙⵚⵛⵜⵝⵞⵟⵠⵡⵢⵣⵤⵥ\n",
    "    \n",
    "    2. Latin script adaptations for Amazigh languages (with special characters like ɣ, ḥ, ṛ, ṣ, ṭ, ẓ)\n",
    "    \n",
    "    3. Arabic script adaptations used in some Amazigh communities\n",
    "    \n",
    "    Extract all text accurately, preserving the original script and diacritical marks.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c89dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "def setup_model_and_processor(model_name: str = \"Qwen/Qwen2-VL-2B\"):\n",
    "    \"\"\"Setup the model and processor with LoRA + 4-bit quantization\"\"\"\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForImageTextToText.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01f1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Setting up model and processor...\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.16s/it]\n",
      "INFO:__main__:Loading dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,179,072 || all params: 2,211,164,672 || trainable%: 0.0985\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Setting up model and processor...\")\n",
    "model, processor = setup_model_and_processor()\n",
    "\n",
    "logger.info(\"Loading dataset...\")\n",
    "dataset = AmazighOCRDataset(\"images\", processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae8c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26090c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_collator(processor):\n",
    "    \"\"\"Create a custom data collator for VL models\"\"\"\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        # Extract components\n",
    "        input_ids = [item['input_ids'] for item in batch]\n",
    "        attention_masks = [item['attention_mask'] for item in batch]\n",
    "        pixel_values = [item['pixel_values'] for item in batch]\n",
    "        labels = [item['labels'] for item in batch]\n",
    "        \n",
    "        # Pad sequences\n",
    "        max_len = max(len(ids) for ids in input_ids)\n",
    "        \n",
    "        padded_input_ids = []\n",
    "        padded_attention_masks = []\n",
    "        padded_labels = []\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            pad_len = max_len - len(input_ids[i])\n",
    "            \n",
    "            padded_input_ids.append(\n",
    "                torch.cat([input_ids[i], torch.full((pad_len,), processor.tokenizer.pad_token_id)])\n",
    "            )\n",
    "            padded_attention_masks.append(\n",
    "                torch.cat([attention_masks[i], torch.zeros(pad_len)])\n",
    "            )\n",
    "            padded_labels.append(\n",
    "                torch.cat([labels[i], torch.full((pad_len,), -100)])  # -100 for ignored tokens\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.stack(padded_input_ids).long(),\n",
    "            'attention_mask': torch.stack(padded_attention_masks),\n",
    "            'pixel_values': torch.stack(pixel_values),\n",
    "            'labels': torch.stack(padded_labels)\n",
    "        }\n",
    "    \n",
    "    return collate_fn\n",
    "\n",
    "def train_amazigh_ocr_model(\n",
    "    data_path: str,\n",
    "    output_dir: str = \"./amazigh-ocr-qwen2vl\",\n",
    "    num_epochs: int = 3,\n",
    "    batch_size: int = 2,\n",
    "    learning_rate: float = 5e-5,\n",
    "    save_steps: int = 500\n",
    "):\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    \n",
    "    logger.info(\"Setting up model and processor...\")\n",
    "    model, processor = setup_model_and_processor()\n",
    "    \n",
    "    logger.info(\"Loading dataset...\")\n",
    "    dataset = AmazighOCRDataset(data_path, processor)\n",
    "    \n",
    "    # Split dataset (80/20 train/val)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # Setup training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = batch_size * 4\n",
    "        warmup_steps=50,\n",
    "        learning_rate=learning_rate,\n",
    "        fp16=False,  # Mixed precision training\n",
    "        logging_steps=50,\n",
    "        save_steps=save_steps,\n",
    "        eval_steps=save_steps,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=None,  # Disable wandb/tensorboard\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    # Create data collator\n",
    "    data_collator = create_data_collator(processor)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=processor.tokenizer,\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    logger.info(\"Saving final model...\")\n",
    "    trainer.save_model()\n",
    "    processor.save_pretrained(output_dir)\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "def prepare_dataset_example():\n",
    "    \"\"\"Example function showing how to prepare your dataset\"\"\"\n",
    "    \n",
    "    example_data = [\n",
    "        {\n",
    "            \"image_path\": \"/path/to/tifinagh_text_image.jpg\",\n",
    "            \"text\": \"ⴰⵙⵉⵡⵍ ⵏ ⵜⵎⴰⵣⵉⵖⵜ\"  # Example Tifinagh text\n",
    "        },\n",
    "        {\n",
    "            \"image_path\": \"/path/to/latin_amazigh_image.jpg\", \n",
    "            \"text\": \"tamaziɣt n umaziɣ\"  # Example Latin script Amazigh\n",
    "        },\n",
    "        {\n",
    "            \"image_path\": \"/path/to/arabic_amazigh_image.jpg\",\n",
    "            \"text\": \"تامازيغت\"  # Example Arabic script adaptation\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Save as JSON\n",
    "    with open('amazigh_ocr_dataset.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(example_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"Example dataset created: amazigh_ocr_dataset.json\")\n",
    "    print(\"Replace with your actual image paths and corresponding text\")\n",
    "\n",
    "def inference_example(model_path: str, image_path: str):\n",
    "    \"\"\"Example inference function\"\"\"\n",
    "    \n",
    "    # Load fine-tuned model\n",
    "    model = AutoModelForImageTextToText.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(model_path)\n",
    "    \n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Prepare input\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": \"Extract all text from this image, including Amazigh text in Tifinagh, Latin, or Arabic scripts:\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    text = processor.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=[image],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            temperature=0.0\n",
    "        )\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = processor.batch_decode(\n",
    "        output[:, inputs['input_ids'].shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "    \n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6652f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Setting up model and processor...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer = train_amazigh_ocr_model(\n",
    "    data_path=\"images\",\n",
    "    output_dir=\"./amazigh-ocr-qwen2vl-f inetuned\",\n",
    "    num_epochs=5,\n",
    "    batch_size=1,  # Reduce if memory issues\n",
    "    learning_rate=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc00827",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = inference_example(\n",
    "    model_path=\"./amazigh-ocr-qwen2vl-finetuned\",\n",
    "    image_path=\"path/to/test_image.jpg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f214e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    \n",
    "    # 1. Prepare example dataset structure\n",
    "    print(\"Creating example dataset structure...\")\n",
    "    prepare_dataset_example()\n",
    "    \n",
    "    # 2. Train the model (uncomment when you have real data)\n",
    "    # print(\"Starting training...\")\n",
    "    # trainer = train_amazigh_ocr_model(\n",
    "    #     data_path=\"amazigh_ocr_dataset.json\",\n",
    "    #     output_dir=\"./amazigh-ocr-qwen2vl-finetuned\",\n",
    "    #     num_epochs=5,\n",
    "    #     batch_size=1,  # Reduce if memory issues\n",
    "    #     learning_rate=1e-5\n",
    "    # )\n",
    "    \n",
    "    # 3. Test inference (after training)\n",
    "    # \n",
    "    # print(f\"OCR Result: {result}\")\n",
    "    \n",
    "    print(\"\\nSetup complete! Follow these steps:\")\n",
    "    print(\"1. Prepare your Amazigh text images and corresponding text files\")\n",
    "    print(\"2. Update the dataset paths in the training function\")\n",
    "    print(\"3. Run the training with: python amazigh_ocr_finetune.py\")\n",
    "    print(\"4. Use the trained model for OCR inference\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
